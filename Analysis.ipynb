{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/user1/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/user1/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/user1/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/user1/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/user1/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/user1/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline  \n",
    "from IPython.display import display, HTML, SVG\n",
    "from db import Result\n",
    "import papermill as pm\n",
    "import os\n",
    "import seaborn\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict \n",
    "from sklearn.metrics import auc, precision_recall_curve, roc_curve, confusion_matrix, roc_auc_score, roc_curve, f1_score, accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from evaluate import calculate_confusion_matrix_stats_predictions, calculate_roc_curve, roc_auc_score\n",
    "from scipy.stats import binom_test, fisher_exact, chi2_contingency\n",
    "from tabulate import tabulate\n",
    "from calculate_features import all_features\n",
    "from config import config\n",
    "from data import data\n",
    "plt.rcParams['svg.fonttype'] = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP = False\n",
    "UUIDS = [    \n",
    "    \"0135d149-054c-4be3-aef5-0a72082ae30e\", # t1post\n",
    "    \"02034358-a367-49f0-b761-aa833c094e13\", # t2\n",
    "    \"32d16bb7-2512-4abf-9126-a2d7a4a67c2e\", #features\n",
    "     #\"eff10630-0fc5-495f-aeb6-30d5698ef845\", #features\n",
    "]\n",
    "SELECTIONS = {\n",
    "    # old selections\n",
    "}\n",
    "TSNE_PERPLEXITY = {\n",
    "    \"0135d149-054c-4be3-aef5-0a72082ae30e\" : 10, # t1post\n",
    "    \"02034358-a367-49f0-b761-aa833c094e13\": 10, # t2\n",
    "    \"32d16bb7-2512-4abf-9126-a2d7a4a67c2e\" : 10, #features\n",
    "     #\"eff10630-0fc5-495f-aeb6-30d5698ef845\" : 10, #features\n",
    "}\n",
    "MODALITY = {\n",
    "     \"0135d149-054c-4be3-aef5-0a72082ae30e\": \"t1\", # t1\n",
    "     \"02034358-a367-49f0-b761-aa833c094e13\": \"t2\", # t2    \n",
    "     \"32d16bb7-2512-4abf-9126-a2d7a4a67c2e\": \"features\", # features\n",
    "    #\"eff10630-0fc5-495f-aeb6-30d5698ef845\": \"features\", # features\"\n",
    "}\n",
    "old_modality = {\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def adjusted_wald(p, n, z=1.96):\n",
    "    p_adj = (n * p + (z**2)/2)/(n+z**2)\n",
    "    n_adj = n + z**2\n",
    "    span = z * math.sqrt(p_adj*(1-p_adj)/n_adj)\n",
    "    return max(0, p_adj - span), min(p_adj + span, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_notebook(uuid, selections=None, tsne_perplexity=5, input_form=\"none\", description=\"\"): \n",
    "    name = \"evaluate-{}-{}-{}.ipynb\".format(description, input_form, uuid)\n",
    "    if not SKIP and not os.path.exists(name): \n",
    "        pm.execute_notebook(\n",
    "            #\"evaluate-specific-model.ipynb\",\n",
    "            \"evaluate_2_5D.ipynb\",\n",
    "            \"evaluate-{}-{}-{}.ipynb\".format(description, input_form, uuid),\n",
    "            parameters = dict(\n",
    "                UUID=uuid,\n",
    "                SELECTIONS=repr(selections),\n",
    "                TSNE_PERPLEXITY=tsne_perplexity,\n",
    "            ),\n",
    "    )\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_notebooks = list()\n",
    "modality_by_notebook = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0135d149-054c-4be3-aef5-0a72082ae30e\n",
      "done\n",
      "02034358-a367-49f0-b761-aa833c094e13\n",
      "done\n",
      "32d16bb7-2512-4abf-9126-a2d7a4a67c2e\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "for uuid in UUIDS: \n",
    "    print(uuid)\n",
    "    result = Result.query.filter(Result.uuid == uuid).first()\n",
    "    name = execute_notebook(uuid, SELECTIONS.get(uuid), TSNE_PERPLEXITY.get(uuid), result.input_form, result.description)\n",
    "    completed_notebooks.append(name)\n",
    "    modality_by_notebook[name] = MODALITY[uuid]\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'evaluate-effNet-t1-0135d149-054c-4be3-aef5-0a72082ae30e.ipynb': 't1',\n",
       " 'evaluate-effNet-t2-02034358-a367-49f0-b761-aa833c094e13.ipynb': 't2',\n",
       " 'evaluate-effNet-features-32d16bb7-2512-4abf-9126-a2d7a4a67c2e.ipynb': 'features'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modality_by_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Flatten, Activation, Dropout\n",
    "\n",
    "def swish_activation(x):\n",
    "        return (K.sigmoid(x) * x)\n",
    "\n",
    "tf.keras.utils.get_custom_objects().update({'swish_activation': Activation(swish_activation)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d023f7c09454f539a14ade341d362d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=47), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "PapermillExecutionError",
     "evalue": "\n---------------------------------------------------------------------------\nException encountered at \"In [6]\":\n---------------------------------------------------------------------------\nInternalError                             Traceback (most recent call last)\n<ipython-input-6-04599cb37f4d> in <module>\n----> 1 sd = stacked_data(uuids=models)\n\n~/robin/ovarian/ovarian/stacked_data.py in stacked_data(uuids, epochs)\n    120     test_predictions = list()\n    121     for result in results:\n--> 122         model = load(result)\n    123         t = training[result.input_form]\n    124         tf = training_fixed[result.input_form]\n\n~/robin/ovarian/ovarian/stacked_data.py in load(result)\n     17 def load(result):\n     18     path = \"{}/models/{}-{}.h5\".format(config.OUTPUT, result.uuid, result.model)\n---> 19     return load_model(path)\n     20 \n     21 def xload(result):\n\n~/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/keras/engine/saving.py in load_wrapper(*args, **kwargs)\n    456                 os.remove(tmp_filepath)\n    457             return res\n--> 458         return load_function(*args, **kwargs)\n    459 \n    460     return load_wrapper\n\n~/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/keras/engine/saving.py in load_model(filepath, custom_objects, compile)\n    548     if H5Dict.is_supported_type(filepath):\n    549         with H5Dict(filepath, mode='r') as h5dict:\n--> 550             model = _deserialize_model(h5dict, custom_objects, compile)\n    551     elif hasattr(filepath, 'write') and callable(filepath.write):\n    552         def load_function(h5file):\n\n~/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/keras/engine/saving.py in _deserialize_model(h5dict, custom_objects, compile)\n    241         raise ValueError('No model found in config.')\n    242     model_config = json.loads(model_config.decode('utf-8'))\n--> 243     model = model_from_config(model_config, custom_objects=custom_objects)\n    244     model_weights_group = h5dict['model_weights']\n    245 \n\n~/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/keras/engine/saving.py in model_from_config(config, custom_objects)\n    591                         '`Sequential.from_config(config)`?')\n    592     from ..layers import deserialize\n--> 593     return deserialize(config, custom_objects=custom_objects)\n    594 \n    595 \n\n~/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/keras/layers/__init__.py in deserialize(config, custom_objects)\n    166                                     module_objects=globs,\n    167                                     custom_objects=custom_objects,\n--> 168                                     printable_module_name='layer')\n\n~/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/keras/utils/generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)\n    145                     config['config'],\n    146                     custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n--> 147                                         list(custom_objects.items())))\n    148             with CustomObjectScope(custom_objects):\n    149                 return cls.from_config(config['config'])\n\n~/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/keras/engine/network.py in from_config(cls, config, custom_objects)\n   1060                         node_data = node_data_list[node_index]\n   1061                         try:\n-> 1062                             process_node(layer, node_data)\n   1063 \n   1064                         # If the node does not have all inbound layers\n\n~/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/keras/engine/network.py in process_node(layer, node_data)\n   1010             # and building the layer if needed.\n   1011             if input_tensors:\n-> 1012                 layer(unpack_singleton(input_tensors), **kwargs)\n   1013 \n   1014         def process_layer(layer_data):\n\n~/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/keras/engine/base_layer.py in __call__(self, inputs, **kwargs)\n    449             # Actually call the layer,\n    450             # collecting output(s), mask(s), and shape(s).\n--> 451             output = self.call(inputs, **kwargs)\n    452             output_mask = self.compute_mask(inputs, previous_mask)\n    453 \n\n~/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/keras/layers/normalization.py in call(self, inputs, training)\n    183         normed_training, mean, variance = K.normalize_batch_in_training(\n    184             inputs, self.gamma, self.beta, reduction_axes,\n--> 185             epsilon=self.epsilon)\n    186 \n    187         if K.backend() != 'cntk':\n\n~/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py in normalize_batch_in_training(x, gamma, beta, reduction_axes, epsilon)\n   2063     \"\"\"\n   2064     if ndim(x) == 4 and list(reduction_axes) in [[0, 1, 2], [0, 2, 3]]:\n-> 2065         if not _has_nchw_support() and list(reduction_axes) == [0, 2, 3]:\n   2066             return _broadcast_normalize_batch_in_training(x, gamma, beta,\n   2067                                                           reduction_axes,\n\n~/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py in _has_nchw_support()\n    310     \"\"\"\n    311     explicitly_on_cpu = _is_current_explicit_device('CPU')\n--> 312     gpus_available = len(_get_available_gpus()) > 0\n    313     return (not explicitly_on_cpu and gpus_available)\n    314 \n\n~/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py in _get_available_gpus()\n    294     global _LOCAL_DEVICES\n    295     if _LOCAL_DEVICES is None:\n--> 296         _LOCAL_DEVICES = get_session().list_devices()\n    297     return [x.name for x in _LOCAL_DEVICES if x.device_type == 'GPU']\n    298 \n\n~/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py in get_session()\n    201                                         inter_op_parallelism_threads=num_thread,\n    202                                         allow_soft_placement=True)\n--> 203             _SESSION = tf.Session(config=config)\n    204         session = _SESSION\n    205     if not _MANUAL_VAR_INIT:\n\n~/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/tensorflow/python/client/session.py in __init__(self, target, graph, config)\n   1549 \n   1550     \"\"\"\n-> 1551     super(Session, self).__init__(target, graph, config=config)\n   1552     # NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.\n   1553     self._default_graph_context_manager = None\n\n~/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/tensorflow/python/client/session.py in __init__(self, target, graph, config)\n    674     try:\n    675       # pylint: disable=protected-access\n--> 676       self._session = tf_session.TF_NewSessionRef(self._graph._c_graph, opts)\n    677       # pylint: enable=protected-access\n    678     finally:\n\nInternalError: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 34055847936\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPapermillExecutionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-fa2e87c7f050>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         parameters = dict(\n\u001b[1;32m      6\u001b[0m             \u001b[0mMODELS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mUUIDS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mSCORE\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         ),\n\u001b[1;32m      9\u001b[0m     )\n",
      "\u001b[0;32m~/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/papermill/execute.py\u001b[0m in \u001b[0;36mexecute_notebook\u001b[0;34m(input_path, output_path, parameters, engine_name, prepare_only, kernel_name, progress_bar, log_output, start_timeout, report_mode, cwd)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;31m# Check for errors first (it saves on error before raising)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mraise_for_execution_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# Write final output in case the engine didn't write it on cell completion.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/papermill/execute.py\u001b[0m in \u001b[0;36mraise_for_execution_errors\u001b[0;34m(nb, output_path)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcells\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0merror_msg_cell\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcells\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mwrite_ipynb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mPapermillExecutionError\u001b[0m: \n---------------------------------------------------------------------------\nException encountered at \"In [6]\":\n---------------------------------------------------------------------------\nInternalError                             Traceback (most recent call last)\n<ipython-input-6-04599cb37f4d> in <module>\n----> 1 sd = stacked_data(uuids=models)\n\n~/robin/ovarian/ovarian/stacked_data.py in stacked_data(uuids, epochs)\n    120     test_predictions = list()\n    121     for result in results:\n--> 122         model = load(result)\n    123         t = training[result.input_form]\n    124         tf = training_fixed[result.input_form]\n\n~/robin/ovarian/ovarian/stacked_data.py in load(result)\n     17 def load(result):\n     18     path = \"{}/models/{}-{}.h5\".format(config.OUTPUT, result.uuid, result.model)\n---> 19     return load_model(path)\n     20 \n     21 def xload(result):\n\n~/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/keras/engine/saving.py in load_wrapper(*args, **kwargs)\n    456                 os.remove(tmp_filepath)\n    457             return res\n--> 458         return load_function(*args, **kwargs)\n    459 \n    460     return load_wrapper\n\n~/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/keras/engine/saving.py in load_model(filepath, custom_objects, compile)\n    548     if H5Dict.is_supported_type(filepath):\n    549         with H5Dict(filepath, mode='r') as h5dict:\n--> 550             model = _deserialize_model(h5dict, custom_objects, compile)\n    551     elif hasattr(filepath, 'write') and callable(filepath.write):\n    552         def load_function(h5file):\n\n~/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/keras/engine/saving.py in _deserialize_model(h5dict, custom_objects, compile)\n    241         raise ValueError('No model found in config.')\n    242     model_config = json.loads(model_config.decode('utf-8'))\n--> 243     model = model_from_config(model_config, custom_objects=custom_objects)\n    244     model_weights_group = h5dict['model_weights']\n    245 \n\n~/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/keras/engine/saving.py in model_from_config(config, custom_objects)\n    591                         '`Sequential.from_config(config)`?')\n    592     from ..layers import deserialize\n--> 593     return deserialize(config, custom_objects=custom_objects)\n    594 \n    595 \n\n~/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/keras/layers/__init__.py in deserialize(config, custom_objects)\n    166                                     module_objects=globs,\n    167                                     custom_objects=custom_objects,\n--> 168                                     printable_module_name='layer')\n\n~/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/keras/utils/generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)\n    145                     config['config'],\n    146                     custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n--> 147                                         list(custom_objects.items())))\n    148             with CustomObjectScope(custom_objects):\n    149                 return cls.from_config(config['config'])\n\n~/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/keras/engine/network.py in from_config(cls, config, custom_objects)\n   1060                         node_data = node_data_list[node_index]\n   1061                         try:\n-> 1062                             process_node(layer, node_data)\n   1063 \n   1064                         # If the node does not have all inbound layers\n\n~/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/keras/engine/network.py in process_node(layer, node_data)\n   1010             # and building the layer if needed.\n   1011             if input_tensors:\n-> 1012                 layer(unpack_singleton(input_tensors), **kwargs)\n   1013 \n   1014         def process_layer(layer_data):\n\n~/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/keras/engine/base_layer.py in __call__(self, inputs, **kwargs)\n    449             # Actually call the layer,\n    450             # collecting output(s), mask(s), and shape(s).\n--> 451             output = self.call(inputs, **kwargs)\n    452             output_mask = self.compute_mask(inputs, previous_mask)\n    453 \n\n~/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/keras/layers/normalization.py in call(self, inputs, training)\n    183         normed_training, mean, variance = K.normalize_batch_in_training(\n    184             inputs, self.gamma, self.beta, reduction_axes,\n--> 185             epsilon=self.epsilon)\n    186 \n    187         if K.backend() != 'cntk':\n\n~/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py in normalize_batch_in_training(x, gamma, beta, reduction_axes, epsilon)\n   2063     \"\"\"\n   2064     if ndim(x) == 4 and list(reduction_axes) in [[0, 1, 2], [0, 2, 3]]:\n-> 2065         if not _has_nchw_support() and list(reduction_axes) == [0, 2, 3]:\n   2066             return _broadcast_normalize_batch_in_training(x, gamma, beta,\n   2067                                                           reduction_axes,\n\n~/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py in _has_nchw_support()\n    310     \"\"\"\n    311     explicitly_on_cpu = _is_current_explicit_device('CPU')\n--> 312     gpus_available = len(_get_available_gpus()) > 0\n    313     return (not explicitly_on_cpu and gpus_available)\n    314 \n\n~/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py in _get_available_gpus()\n    294     global _LOCAL_DEVICES\n    295     if _LOCAL_DEVICES is None:\n--> 296         _LOCAL_DEVICES = get_session().list_devices()\n    297     return [x.name for x in _LOCAL_DEVICES if x.device_type == 'GPU']\n    298 \n\n~/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py in get_session()\n    201                                         inter_op_parallelism_threads=num_thread,\n    202                                         allow_soft_placement=True)\n--> 203             _SESSION = tf.Session(config=config)\n    204         session = _SESSION\n    205     if not _MANUAL_VAR_INIT:\n\n~/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/tensorflow/python/client/session.py in __init__(self, target, graph, config)\n   1549 \n   1550     \"\"\"\n-> 1551     super(Session, self).__init__(target, graph, config=config)\n   1552     # NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.\n   1553     self._default_graph_context_manager = None\n\n~/Envs/ovarian-ZJ9fDbrR/lib/python3.6/site-packages/tensorflow/python/client/session.py in __init__(self, target, graph, config)\n    674     try:\n    675       # pylint: disable=protected-access\n--> 676       self._session = tf_session.TF_NewSessionRef(self._graph._c_graph, opts)\n    677       # pylint: enable=protected-access\n    678     finally:\n\nInternalError: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 34055847936\n"
     ]
    }
   ],
   "source": [
    "if not SKIP: \n",
    "    pm.execute_notebook(\n",
    "        \"evaluate-ensemble.ipynb\",\n",
    "        \"evaluate-ensemble-{}.ipynb\".format(\"-\".join(UUIDS)),\n",
    "        parameters = dict(\n",
    "            MODELS=UUIDS,\n",
    "            SCORE=\"accuracy\",\n",
    "        ),\n",
    "    )\n",
    "completed_notebooks.append(\"evaluate-ensemble-{}.ipynb\".format(\"-\".join(UUIDS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modality_by_notebook[\"evaluate-ensemble-{}.ipynb\".format(\"-\".join(UUIDS))] = \"ensemble\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notebook_output(notebook, name): \n",
    "    return notebook.dataframe[notebook.dataframe.name==name].value.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dict()\n",
    "validation = dict()\n",
    "test = dict()\n",
    "for notebook in completed_notebooks: \n",
    "    nb = pm.read_notebook(notebook)\n",
    "    train[\"{}-{}\".format(modality_by_notebook[notebook], \"labels\")] = get_notebook_output(nb, \"train_labels\")\n",
    "    train[\"{}-{}\".format(modality_by_notebook[notebook], \"predictions\")] = get_notebook_output(nb, \"train_predictions\")\n",
    "    train[\"{}-{}\".format(modality_by_notebook[notebook], \"probabilities\")] = get_notebook_output(nb, \"train_probabilities\")\n",
    "    validation[\"{}-{}\".format(modality_by_notebook[notebook], \"labels\")] = get_notebook_output(nb, \"validation_labels\")\n",
    "    validation[\"{}-{}\".format(modality_by_notebook[notebook], \"predictions\")] = get_notebook_output(nb, \"validation_predictions\")\n",
    "    validation[\"{}-{}\".format(modality_by_notebook[notebook], \"probabilities\")] = get_notebook_output(nb, \"validation_probabilities\")\n",
    "    test[\"{}-{}\".format(modality_by_notebook[notebook], \"labels\")] = get_notebook_output(nb, \"test_labels\")\n",
    "    test[\"{}-{}\".format(modality_by_notebook[notebook], \"predictions\")] = get_notebook_output(nb, \"test_predictions\")\n",
    "    test[\"{}-{}\".format(modality_by_notebook[notebook], \"probabilities\")] = get_notebook_output(nb, \"test_probabilities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "result = Result.query.filter(Result.uuid == UUIDS[0]).first()\n",
    "train_set, validation_set, test_set = data(seed=uuid.UUID(result.split_seed), label_form=result.label_form, input_form=result.input_form, train_shuffle=False, test_shuffle=False, validation_shuffle=False, train_augment=False, validation_augment=False, test_augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODALITIES = [\n",
    "    \"features\",     \n",
    "#    \"t1-sensitive\",\n",
    "    \"t2\",\n",
    "    \"t1\",\n",
    "#    \"t2-specific\",\n",
    "    \"ensemble\", \n",
    "]\n",
    "\n",
    "MODALITY_KEY = {\n",
    "    \"features\": \"Clinical\",     \n",
    "    \"t1\": \"T1C\", \n",
    "    \"t2\": \"T2WI\", \n",
    "    \"t1-sensitive\": \"T1C Sensitive\", \n",
    "    \"t2-sensitive\": \"T2WI Sensitive\",     \n",
    "    \"t1-specific\": \"T1C Specific\", \n",
    "    \"t2-specific\": \"T2WI Specific\",         \n",
    "    \"ensemble\": \"Ensemble\", \n",
    "}\n",
    "\n",
    "def get_pr_data_for_modality(dataset, comparison_models=[]): \n",
    "    results = list()\n",
    "    points = list()\n",
    "    for modality in MODALITIES: \n",
    "        labels = dataset[\"{}-labels\".format(modality)]\n",
    "        probabilities = dataset[\"{}-probabilities\".format(modality)]\n",
    "        predictions = dataset[\"{}-predictions\".format(modality)]\n",
    "        print(modality, len(labels), len(probabilities), len(predictions))\n",
    "        acc = accuracy_score(labels, predictions)\n",
    "        precision, recall, _ = precision_recall_curve(labels, probabilities)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        stats = calculate_confusion_matrix_stats_predictions(labels, predictions)\n",
    "        points.append({\n",
    "            \"Modality\": \"{} (auc={:.2f}, acc={:.2f})\".format(MODALITY_KEY[modality], pr_auc, acc),\n",
    "            \"precision\": stats[\"PPV\"][1],\n",
    "            \"recall\": stats[\"TPR\"][1],\n",
    "        })\n",
    "        for p, r in zip(precision, recall): \n",
    "            results.append({ \"precision\": p, \"recall\": r, \"Modality\": \"{} (auc={:.2f}, acc={:.2f})\".format(MODALITY_KEY[modality], pr_auc, acc)})\n",
    "    for probabilities in comparison_models: \n",
    "        modality = \"Radiomics\"\n",
    "        labels = dataset[\"t1-labels\"]\n",
    "        predictions = [p > 0.5 for p in probabilities]\n",
    "        print(modality, len(labels), len(probabilities), len(predictions))\n",
    "        precision, recall, _ = precision_recall_curve(labels, probabilities)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        stats = calculate_confusion_matrix_stats_predictions(labels, predictions)\n",
    "        acc = accuracy_score(labels, predictions)\n",
    "        points.append({\n",
    "            \"Modality\": \"{} (auc={:.2f}, acc={:.2f})\".format(MODALITY_KEY[modality], pr_auc, acc),\n",
    "            \"precision\": stats[\"PPV\"][1],\n",
    "            \"recall\": stats[\"TPR\"][1],\n",
    "        })\n",
    "        for p, r in zip(precision, recall): \n",
    "            results.append({ \"precision\": p, \"recall\": r, \"Modality\": \"{} (auc={:.2f}, acc={:.2f})\".format(MODALITY_KEY[modality], pr_auc, acc)})           \n",
    "    return results, pr_auc, []\n",
    "        \n",
    "def plot_multiple_precision_recall(dataset, experts=[], comparison_models=[]):\n",
    "    results, auc, points = get_pr_data_for_modality(dataset, comparison_models)        \n",
    "    if len(experts) > 0:\n",
    "        for i, expert in enumerate(experts): \n",
    "            labels = dataset[\"t1-labels\"]\n",
    "            predictions = expert\n",
    "            stats = calculate_confusion_matrix_stats_predictions(labels, predictions)\n",
    "            acc = accuracy_score(labels, predictions)\n",
    "            if i == 4:\n",
    "                points.append({\n",
    "                \"precision\": stats[\"PPV\"][1],\n",
    "                \"recall\": stats[\"TPR\"][1],                \n",
    "                \"Other\": \"Radiomics-GLM (acc={:.2f})\".format(acc), \n",
    "                })\n",
    "            elif i == 5:\n",
    "                points.append({\n",
    "                \"precision\": stats[\"PPV\"][1],\n",
    "                \"recall\": stats[\"TPR\"][1],                \n",
    "                \"Other\": \"Radiomics-NNet (acc={:.2f})\".format(acc), \n",
    "                })\n",
    "            else:\n",
    "                points.append({\n",
    "                \"precision\": stats[\"PPV\"][1],\n",
    "                \"recall\": stats[\"TPR\"][1],                \n",
    "                \"Other\": \"Expert {} (acc={:.2f})\".format(i + 1, acc), \n",
    "                })\n",
    "    fig, ax = plt.subplots()\n",
    "    seaborn.lineplot(\n",
    "        data=pandas.DataFrame(results),\n",
    "        x=\"recall\",\n",
    "        y=\"precision\",\n",
    "        hue=\"Modality\",\n",
    "        ax=ax, \n",
    "        err_style=None,\n",
    "    )\n",
    "    if points: \n",
    "        seaborn.scatterplot(\n",
    "            data=pandas.DataFrame(points),\n",
    "            x=\"recall\",\n",
    "            y=\"precision\",\n",
    "            hue=\"Other\",\n",
    "            style=\"Other\",                        \n",
    "            markers=[\"o\", \"v\", \"s\", \"P\", \"D\", \"^\"],\n",
    "            palette={ p[\"Other\"]: \"black\" for p in points },            \n",
    "            ax=ax,\n",
    "            s=50\n",
    "        )\n",
    "    ax.set_ylim(0.2, 1.04)\n",
    "    ax.set_xlim(-0.04, 1.02)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    return fig\n",
    "\n",
    "def get_roc_data_for_modality(dataset, comparison_models=[]): \n",
    "    results = list()\n",
    "    points = list()\n",
    "    for modality in MODALITIES: \n",
    "        labels = dataset[\"{}-labels\".format(modality)]\n",
    "        probabilities = dataset[\"{}-probabilities\".format(modality)]\n",
    "        predictions = dataset[\"{}-predictions\".format(modality)]\n",
    "        fpr, tpr, _ = roc_curve(labels, probabilities, drop_intermediate=False)\n",
    "        roc_auc = roc_auc_score(labels, probabilities)\n",
    "        stats = calculate_confusion_matrix_stats_predictions(labels, predictions)\n",
    "        acc = accuracy_score(labels, predictions)\n",
    "        points.append({\n",
    "            \"Modality\": \"{} (auc={:.2f}, acc={:.2f})\".format(MODALITY_KEY[modality], roc_auc, acc),\n",
    "            \"fpr\": stats[\"FPR\"][1],\n",
    "            \"tpr\": stats[\"TPR\"][1],\n",
    "        })\n",
    "        for f, t in zip(fpr, tpr): \n",
    "            results.append({ \"fpr\": f, \"tpr\": t, \"Modality\": \"{} (auc={:.2f}, acc={:.2f})\".format(MODALITY_KEY[modality], roc_auc, acc)})\n",
    "    for probabilities in comparison_models: \n",
    "        modality = \"Radiomics\"\n",
    "        labels = dataset[\"t1-labels\"]\n",
    "        predictions = [p > 0.5 for p in probabilities]\n",
    "        fpr, tpr, _ = roc_curve(labels, probabilities, drop_intermediate=False)\n",
    "        roc_auc = roc_auc_score(labels, probabilities)\n",
    "        stats = calculate_confusion_matrix_stats_predictions(labels, predictions)\n",
    "        acc = accuracy_score(labels, predictions)\n",
    "        points.append({\n",
    "            \"Modality\": \"{} (auc={:.2f}, acc={:.2f})\".format(MODALITY_KEY[modality], roc_auc, acc),\n",
    "            \"fpr\": stats[\"FPR\"][1],\n",
    "            \"tpr\": stats[\"TPR\"][1],\n",
    "        })\n",
    "        for f, t in zip(fpr, tpr): \n",
    "            results.append({ \"fpr\": f, \"tpr\": t, \"Modality\": \"{} (auc={:.2f}, acc={:.2f})\".format(MODALITY_KEY[modality], roc_auc, acc)})        \n",
    "    return results, roc_auc, []\n",
    "        \n",
    "def plot_multiple_roc_curve(dataset, experts=[], comparison_models=[]):\n",
    "    results, auc, points = get_roc_data_for_modality(dataset, comparison_models)\n",
    "    if len(experts) > 0:\n",
    "        for i, expert in enumerate(experts): \n",
    "            labels = dataset[\"t1-labels\"]\n",
    "            predictions = expert\n",
    "            stats = calculate_confusion_matrix_stats_predictions(labels, predictions)\n",
    "            acc = accuracy_score(labels, predictions) \n",
    "            if i == 4:\n",
    "                points.append({\n",
    "                \"fpr\": stats[\"FPR\"][1],\n",
    "                \"tpr\": stats[\"TPR\"][1],               \n",
    "                \"Other\": \"Radiomics-GLM (acc={:.2f})\".format(acc), \n",
    "                })\n",
    "            elif i == 5:\n",
    "                points.append({\n",
    "                \"fpr\": stats[\"FPR\"][1],\n",
    "                \"tpr\": stats[\"TPR\"][1],               \n",
    "                \"Other\": \"Radiomics-NNet (acc={:.2f})\".format(acc), \n",
    "                })\n",
    "            else:\n",
    "                points.append({\n",
    "                 \"fpr\": stats[\"FPR\"][1],\n",
    "                \"tpr\": stats[\"TPR\"][1],     \n",
    "                \"Other\": \"Expert {} (acc={:.2f})\".format(i + 1, acc), \n",
    "                })\n",
    "    fig, ax = plt.subplots()\n",
    "    seaborn.lineplot(\n",
    "        data=pandas.DataFrame(results),\n",
    "        x=\"fpr\",\n",
    "        y=\"tpr\",\n",
    "        hue=\"Modality\",\n",
    "        ax=ax,\n",
    "        err_style=None,\n",
    "    )\n",
    "    if points:     \n",
    "        seaborn.scatterplot(\n",
    "            data=pandas.DataFrame(points),\n",
    "            x=\"fpr\",\n",
    "            y=\"tpr\",\n",
    "            hue=\"Other\",\n",
    "            style=\"Other\",            \n",
    "            ax=ax,\n",
    "            markers=[\"o\", \"v\", \"s\", \"P\", \"D\", \"^\"],\n",
    "            palette={ p[\"Other\"]: \"black\" for p in points },\n",
    "            s=50\n",
    "        )\n",
    "    ax.plot([0, 1], [0, 1], linestyle='--')\n",
    "    ax.set_ylim(-0.04, 1.04)\n",
    "    ax.set_xlim(-0.04, 1.02)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    return fig\n",
    "\n",
    "def get_statistics(dataset, experts=[], comparison_models=[]): \n",
    "    results = list()\n",
    "    for modality in MODALITIES: \n",
    "        labels = dataset[\"{}-labels\".format(modality)]\n",
    "        probabilities = dataset[\"{}-probabilities\".format(modality)]\n",
    "        predictions = dataset[\"{}-predictions\".format(modality)]\n",
    "        roc_auc = roc_auc_score(labels, probabilities)\n",
    "        precision, recall, _ = precision_recall_curve(labels, probabilities)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        f1 = f1_score(labels, predictions)\n",
    "        d = {\n",
    "            \"F1 Score\": [f1, f1],            \n",
    "            \"ROC AUC\": [roc_auc, roc_auc], \n",
    "            \"PR AUC\": [pr_auc, pr_auc],\n",
    "            **calculate_confusion_matrix_stats_predictions(labels, predictions), \n",
    "            \"Modality\": [modality.capitalize(), modality.capitalize()], \n",
    "            \"Total\": [len(labels), len(labels)], \n",
    "            \"Malignant\": [len([l for l in labels if l]), len([l for l in labels if l])], \n",
    "            \"Benign\": [len([l for l in labels if not l]), len([l for l in labels if not l])], \n",
    "        } \n",
    "        # remove more that are not relevant to imbalanced datasets\n",
    "        del d[\"TP\"]\n",
    "        del d[\"TN\"]\n",
    "        del d[\"FN\"]\n",
    "        del d[\"FP\"]    \n",
    "        del d[\"AM\"]        \n",
    "        del d[\"GM\"]\n",
    "        del d[\"FPR\"]\n",
    "        del d[\"FNR\"]\n",
    "        d[\"Acc (95% CI)\"] = [\"\", \"{:.2f} ({:.2f}-{:.2f})\".format(d[\"Acc\"][1], *adjusted_wald(d[\"Acc\"][1], len(labels)))]\n",
    "        d[\"TPR (95% CI)\"] = [\"\", \"{:.2f} ({:.2f}-{:.2f})\".format(d[\"TPR\"][1], *adjusted_wald(d[\"TPR\"][1], len([l for l in labels if l])))]\n",
    "        d[\"TNR (95% CI)\"] = [\"\", \"{:.2f} ({:.2f}-{:.2f})\".format(d[\"TNR\"][1], *adjusted_wald(d[\"TNR\"][1], len([l for l in labels if not l])))]\n",
    "        results.append(pandas.DataFrame(d).iloc[[1]])\n",
    "    for i, expert in enumerate(experts): \n",
    "        labels = dataset[\"ensemble-labels\"]\n",
    "        predictions = expert\n",
    "        f1 = f1_score(labels, predictions)        \n",
    "        d = {\n",
    "            \"F1 Score\": [f1, f1],            \n",
    "            **calculate_confusion_matrix_stats_predictions(labels, predictions),             \n",
    "            \"Modality\": [\"Expert {}\".format(i + 1), \"Expert {}\".format(i + 1)], \n",
    "            \"Total\": [len(labels), len(labels)], \n",
    "            \"Malignant\": [len([l for l in labels if l]), len([l for l in labels if l])], \n",
    "            \"Benign\": [len([l for l in labels if not l]), len([l for l in labels if not l])],             \n",
    "        }\n",
    "        # remove more that are not relevant to imbalanced datasets (remove from article too)\n",
    "        del d[\"TP\"]\n",
    "        del d[\"TN\"]\n",
    "        del d[\"FN\"]\n",
    "        del d[\"FP\"]               \n",
    "        del d[\"AM\"]        \n",
    "        del d[\"GM\"]\n",
    "        del d[\"FPR\"]\n",
    "        del d[\"FNR\"]        \n",
    "        d[\"Acc (95% CI)\"] = [\"\", \"{:.2f} ({:.2f}-{:.2f})\".format(d[\"Acc\"][1], *adjusted_wald(d[\"Acc\"][1], len(labels)))]\n",
    "        d[\"TPR (95% CI)\"] = [\"\", \"{:.2f} ({:.2f}-{:.2f})\".format(d[\"TPR\"][1], *adjusted_wald(d[\"TPR\"][1], len([l for l in labels if l])))]\n",
    "        d[\"TNR (95% CI)\"] = [\"\", \"{:.2f} ({:.2f}-{:.2f})\".format(d[\"TNR\"][1], *adjusted_wald(d[\"TNR\"][1], len([l for l in labels if not l])))]        \n",
    "        results.append(pandas.DataFrame(d).iloc[[1]])\n",
    "    for probabilities in comparison_models: \n",
    "        labels = dataset[\"ensemble-labels\"]\n",
    "        predictions = [p > 0.5 for p in probabilities]\n",
    "        roc_auc = roc_auc_score(labels, probabilities)\n",
    "        precision, recall, _ = precision_recall_curve(labels, probabilities)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        f1 = f1_score(labels, predictions)\n",
    "        d = {\n",
    "            \"F1 Score\": [f1, f1],            \n",
    "            \"ROC AUC\": [roc_auc, roc_auc], \n",
    "            \"PR AUC\": [pr_auc, pr_auc],\n",
    "            **calculate_confusion_matrix_stats_predictions(labels, predictions), \n",
    "            \"Modality\": [\"Radiomics\", \"Radiomics\"], \n",
    "            \"Total\": [len(labels), len(labels)], \n",
    "            \"Malignant\": [len([l for l in labels if l]), len([l for l in labels if l])], \n",
    "            \"Benign\": [len([l for l in labels if not l]), len([l for l in labels if not l])],             \n",
    "        } \n",
    "        # remove more that are not relevant to imbalanced datasets (remove from article too)\n",
    "        del d[\"TP\"]\n",
    "        del d[\"TN\"]\n",
    "        del d[\"FN\"]\n",
    "        del d[\"FP\"]         \n",
    "        del d[\"AM\"]        \n",
    "        del d[\"GM\"]\n",
    "        del d[\"FPR\"]\n",
    "        del d[\"FNR\"]\n",
    "        d[\"Acc (95% CI)\"] = [\"\", \"{:.2f} ({:.2f}-{:.2f})\".format(d[\"Acc\"][1], *adjusted_wald(d[\"Acc\"][1], len(labels)))]\n",
    "        d[\"TPR (95% CI)\"] = [\"\", \"{:.2f} ({:.2f}-{:.2f})\".format(d[\"TPR\"][1], *adjusted_wald(d[\"TPR\"][1], len([l for l in labels if l])))]\n",
    "        d[\"TNR (95% CI)\"] = [\"\", \"{:.2f} ({:.2f}-{:.2f})\".format(d[\"TNR\"][1], *adjusted_wald(d[\"TNR\"][1], len([l for l in labels if not l])))]        \n",
    "        results.append(pandas.DataFrame(d).iloc[[1]])\n",
    "\n",
    "    return pandas.concat(results, axis=0, sort=False).set_index(\"Modality\")\n",
    "\n",
    "def get_experts_for_names(features, names, experts=[\"expert1\", \"expert2\"], transform=int, default=0): \n",
    "    result = list()\n",
    "    for e in experts: \n",
    "        expert_results = list()\n",
    "        for n in names: \n",
    "            f = features.get(n, None)\n",
    "            if f is None: \n",
    "                print(\"error, cannot find {}\".format(n))\n",
    "                expert_results.append(default)\n",
    "                continue\n",
    "            r = f.get(e, default)\n",
    "            if r == \"\": \n",
    "                r = 0\n",
    "            r = transform(r)\n",
    "            expert_results.append(r)\n",
    "        result.append(expert_results)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_features = all_features(files = [\"/media/user1/my4TB/robin/ovarian/ovarian_data/csv/Ovarian_Experts.csv\"])\n",
    "#comparison_model_features = all_features(files=[\"features/comparison-models.csv\"])\n",
    "df = get_statistics(train)#, get_experts_for_names(expert_features, train_set.names), get_experts_for_names(comparison_model_features, train_set.names, experts=[\"radiomics\"], transform=float))\n",
    "df = df.drop(['Acc', \"TPR\", \"TNR\", \"Total\", \"Malignant\", \"Benign\"], axis=1)\n",
    "print()\n",
    "print(\"##### Train\")\n",
    "print()\n",
    "print(tabulate(df, tablefmt=\"pipe\", headers=\"keys\", floatfmt=\"#.3g\").replace(\"nan\", \"N/A\").replace(\"Features\", \"Clinical\"))\n",
    "df = get_statistics(validation)#, get_experts_for_names(expert_features, validation_set.names), get_experts_for_names(comparison_model_features, validation_set.names, experts=[\"radiomics\"], transform=float))\n",
    "df = df.drop(['Acc', \"TPR\", \"TNR\", \"Total\", \"Malignant\", \"Benign\"], axis=1)\n",
    "print()\n",
    "print(\"##### Validation\")\n",
    "print()\n",
    "print(tabulate(df, tablefmt=\"pipe\", headers=\"keys\", floatfmt=\"#.3g\").replace(\"nan\", \"N/A\").replace(\"Features\", \"Clinical\"))\n",
    "df = get_statistics(test, get_experts_for_names(expert_features, test_set.names, experts=[\"expert1\", \"expert2\", \"expert3\", \"expert4\", \"radiomics1\", \"radiomics2\"]))#, get_experts_for_names(expert_features, test_set.names, experts=[\"radiomics1\", \"radiomics2\"], transform=float))\n",
    "test_statistics = df\n",
    "df = df.drop(['Acc', \"TPR\", \"TNR\", \"Total\", \"Malignant\", \"Benign\"], axis=1)\n",
    "print()\n",
    "print(\"##### Test\")\n",
    "print()\n",
    "print(tabulate(df, tablefmt=\"pipe\", headers=\"keys\", floatfmt=\"#.2g\").replace(\"nan\", \"N/A\").replace(\"Features\", \"Clinical\"))\n",
    "#df = get_statistics(test_1, get_experts_for_names(expert_features, test_set.names, experts=[\"expert1\", \"expert2\", \"expert3\", \"expert4\"]), get_experts_for_names(comparison_model_features, test_set.names, experts=[\"radiomics\"], transform=float))\n",
    "#test_1_statistics = df\n",
    "#df = df.drop(['Acc', \"TPR\", \"TNR\", \"Total\", \"Malignant\", \"Benign\"], axis=1)\n",
    "#print()\n",
    "#print(\"##### Test 1\")\n",
    "#print()\n",
    "#print(tabulate(df, tablefmt=\"pipe\", headers=\"keys\", floatfmt=\"#.2g\").replace(\"nan\", \"N/A\").replace(\"Features\", \"Clinical\"))\n",
    "#df = get_statistics(test_2, get_experts_for_names(expert_features, test_set.names, experts=[\"expert1\", \"expert2\", \"expert3\", \"expert4\"]), get_experts_for_names(comparison_model_features, test_set.names, experts=[\"radiomics\"], transform=float))\n",
    "#test_2_statistics = df\n",
    "#df = df.drop(['Acc', \"TPR\", \"TNR\", \"Total\", \"Malignant\", \"Benign\"], axis=1)\n",
    "#print()\n",
    "#print(\"##### Test 2\")\n",
    "#print()\n",
    "#print(tabulate(df, tablefmt=\"pipe\", headers=\"keys\", floatfmt=\"#.2g\").replace(\"nan\", \"N/A\").replace(\"Features\", \"Clinical\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation_statistics = {\n",
    " 'F1 Score': 0.88,\n",
    " 'ROC AUC': 0.6243243243243244,\n",
    " 'PR AUC': 0.90,\n",
    " 'Acc': 0.80,\n",
    " 'TPR': 0.96,\n",
    " 'TNR': 0.22,\n",
    " 'PPV': 0.896551724137931,\n",
    " 'NPV': 0.3888888888888889,\n",
    " 'FDR': 0.10344827586206896,\n",
    " 'Total': 47,\n",
    " 'Malignant': 37,\n",
    " 'Benign': 10,\n",
    " 'Acc (95% CI)': '0.70 (0.56-0.81)',\n",
    " 'TPR (95% CI)': '0.70 (0.54-0.83)',\n",
    " 'TNR (95% CI)': '0.70 (0.39-0.90)'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CI_text = \"\"\"{value:.2g} (95% CI: {low:.2g}-{high:.2g})\"\"\"\n",
    "\n",
    "statistics_text = \"\"\"test accuracy of {accuracy}, F1 score of {f1:.2g}, and sensitivity of {sensitivity}, and specificity of {specificity}\"\"\"\n",
    "def format_statistics(data):\n",
    "    acc_low, acc_high = adjusted_wald(data[\"Acc\"], data[\"Total\"])\n",
    "    tpr_low, tpr_high = adjusted_wald(data[\"TPR\"], data[\"Malignant\"])\n",
    "    tnr_low, tnr_high = adjusted_wald(data[\"TNR\"], data[\"Benign\"])\n",
    "    data_dict = {\n",
    "        \"accuracy\": CI_text.format(value=data[\"Acc\"], low=acc_low, high=acc_high),\n",
    "        \"sensitivity\": CI_text.format(value=data[\"TPR\"], low=tpr_low, high=tpr_high),\n",
    "        \"specificity\": CI_text.format(value=data[\"TNR\"], low=tnr_low, high=tnr_high),\n",
    "        \"f1\": data[\"F1 Score\"],\n",
    "    }\n",
    "    return statistics_text.format(**data_dict)\n",
    "\n",
    "statistics_AUC_text = \"\"\"test accuracy of {accuracy}, F1 score of {f1:.2g}, precision recall AUC of {PR_AUC:.2g}, sensitivity of {sensitivity}, and specificity of {specificity}\"\"\"\n",
    "def format_AUC_statistics(data):\n",
    "    acc_low, acc_high = adjusted_wald(data[\"Acc\"], data[\"Total\"])\n",
    "    tpr_low, tpr_high = adjusted_wald(data[\"TPR\"], data[\"Malignant\"])\n",
    "    tnr_low, tnr_high = adjusted_wald(data[\"TNR\"], data[\"Benign\"])\n",
    "    data_dict = {\n",
    "        \"accuracy\": CI_text.format(value=data[\"Acc\"], low=acc_low, high=acc_high),\n",
    "        \"sensitivity\": CI_text.format(value=data[\"TPR\"], low=tpr_low, high=tpr_high),\n",
    "        \"specificity\": CI_text.format(value=data[\"TNR\"], low=tnr_low, high=tnr_high),\n",
    "        \"f1\": data[\"F1 Score\"],\n",
    "        \"PR_AUC\": data[\"PR AUC\"],\n",
    "    }\n",
    "    return statistics_AUC_text.format(**data_dict)\n",
    "\n",
    "comparison_text =\"\"\"{accuracy_judge} test accuracy ({accuracy_1:.2g} vs. {accuracy_2:.2g}, p={accuracy_p:.2g}), {sensitivity_judge} test sensitivity ({sensitivity_1:.2g} vs. {sensitivity_2:.2g}, p={sensitivity_p:.2g}) and {specificity_judge} test specificity ({specificity_1:.2g} vs. {specificity_2:.2g}, p={specificity_p:.2g})\"\"\"\n",
    "def format_comparison_text(data_1, data_2):\n",
    "    data_dict = {\n",
    "        \"accuracy_judge\": \"higher\" if data_1[\"Acc\"] > data_2[\"Acc\"] else \"lower\",\n",
    "        \"sensitivity_judge\": \"higher\" if data_1[\"TPR\"] > data_2[\"TPR\"] else \"lower\",\n",
    "        \"specificity_judge\": \"higher\" if data_1[\"TNR\"] > data_2[\"TNR\"] else \"lower\",\n",
    "        \"accuracy_1\": data_1[\"Acc\"],\n",
    "        \"specificity_1\": data_1[\"TNR\"],\n",
    "        \"sensitivity_1\": data_1[\"TPR\"],\n",
    "        \"accuracy_2\": data_2[\"Acc\"],\n",
    "        \"specificity_2\": data_2[\"TNR\"],\n",
    "        \"sensitivity_2\": data_2[\"TPR\"],\n",
    "        \"accuracy_p\": binom_test(int(data_1[\"Acc\"] * data_1[\"Total\"]), data_1[\"Total\"], data_2[\"Acc\"]),\n",
    "        \"specificity_p\": binom_test(int(data_1[\"TNR\"] * data_1[\"Benign\"]), data_1[\"Benign\"], data_2[\"TNR\"]),\n",
    "        \"sensitivity_p\": binom_test(int(data_1[\"TPR\"] * data_1[\"Malignant\"]), data_1[\"Malignant\"], data_2[\"TPR\"]),\n",
    "    }\n",
    "    return comparison_text.format(**data_dict)\n",
    "\n",
    "results_text = \"\"\"\n",
    "## Model performance\n",
    "The train, validation and test sets were balanced in terms of age, gender, tumor size, tumor laterality, tumor location, histologic diagnosis, and institution (Supplemental table S1).\n",
    "\n",
    "Performance characteristics of the clinical variable logistic regression, models trained on T1C and T2WI images, and the final ensemble model in test set are summarized in Table 2. Performance characteristics in training and validation sets are summarized in Supplemental table S2. The clinical variable logistic regression achieved a {logistic_regression_statistics}.\n",
    "\n",
    "The T1C trained model achieved a {t1_statistics}. The T2WI trained model achieved a {t2_statistics}. The ensemble model achieved a {ensemble_statistics}. The ensemble model achieved comparative performance on the second set of test set segmentations with a ensemble_1_statistics and the third set of test set segmentation with a ensemble_2_statistics (Supplemental table S3). On average, cross validation analysis of the ensemble model demonstrated a cross_validation_statistics. Supplemental table S4 summarizes the cross-validation performance of the ensemble model.\n",
    "\n",
    "In comparison, expert 1 achieved a expert_1_statistics; expert 2 had a expert_2_statistics; expert 3 had a expert_3_statistics; expert 4 had a expert_4_statistics. Radiomics model achieved a radiomics_statistics (Supplemental table S5).\n",
    "\n",
    "CONFIRM THE BELOW PARAGRAPH IS STILL TRUE\n",
    "\n",
    "Compared to all experts averaged, the ensemble deep learning model had ensemble_vs_experts, COMMENT although none of these statistics was significantly different. Compared to the radiomics model, the ensemble deep learning model had ensemble_vs_radiomics; COMMENT difference in accuracy was not significant but differences in sensitivity and specificity between radiomics and ensemble deep learning models were significant. Compared to all experts averaged, the radiomics model had radiomics_vs_experts; COMMENT difference in accuracy was not significant, but differences in sensitivity and specificity between radiomics and averaged expert performance metrics were significant. Figure 2 shows the precision recall curves of all models overlaid with expert performance.\n",
    "\n",
    "Grad-Cam focus maps demonstrate that for the correctly classified images by our model, the algorithm was more likely to focus on the lesion or part of the lesion deemed important by the radiologists, while focus was more likely to be on the surrounding tissue for the incorrectly classified lesions (Figure 3). t-SNE representation of the final dense layer of ResNet demonstrates good separation of malignant and benign lesions by the model when compared to histopathological diagnosis (Figure 4).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(results_text.format(\n",
    "    logistic_regression_statistics=format_AUC_statistics(dict(test_statistics.loc[\"Features\"])),\n",
    "    t1_statistics=format_AUC_statistics(dict(test_statistics.loc[\"T1\"])),\n",
    "    t2_statistics=format_AUC_statistics(dict(test_statistics.loc[\"T2\"])),\n",
    "    ensemble_statistics=format_AUC_statistics(dict(test_statistics.loc[\"Ensemble\"])),\n",
    "    #cross_validation_statistics=format_AUC_statistics(cross_validation_statistics),\n",
    "    expert_1_statistics=format_statistics(dict(test_statistics.loc[\"Expert 1\"])),\n",
    "    expert_2_statistics=format_statistics(dict(test_statistics.loc[\"Expert 2\"])),\n",
    "    expert_3_statistics=format_statistics(dict(test_statistics.loc[\"Expert 3\"])),\n",
    "    expert_4_statistics=format_statistics(dict(test_statistics.loc[\"Expert 4\"])),\n",
    "    #radiomics_statistics=format_statistics(dict(test_statistics.loc[\"Radiomics\"])),\n",
    "    ensemble_vs_experts=format_comparison_text(dict(test_statistics.loc[\"Ensemble\"]), dict(test_statistics.loc[[\"Expert 1\", \"Expert 2\", \"Expert 3\", \"Expert 4\"]].mean())),\n",
    "    #ensemble_vs_radiomics=format_comparison_text(dict(test_statistics.loc[\"Ensemble\"]), dict(test_statistics.loc[\"Radiomics\"])),\n",
    "    #radiomics_vs_experts=format_comparison_text(dict(test_statistics.loc[\"Radiomics\"]), dict(test_statistics.loc[[\"Expert 1\", \"Expert 2\", \"Expert 3\", \"Expert 4\"]].mean())),\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_text = \"\"\"\n",
    "## Inter-segmenter agreement\n",
    "\n",
    "The average DSC among our three segmenters was {dice_average}. Segmenter 1 and 2 had an average DSC of {dice_1_2}. Segmenter 2 and 3 had an average DSC of {dice_2_3}. Segmenter 1 and 3 had an average DSC of {dice_1_3}. Benign lesions had an average DSC of {dice_benign} across all reviewers, while malignant lesions had an average DSC of {dice_malignant} across all reviewers. \n",
    "\n",
    "## Model performance\n",
    "The train, validation and test sets were balanced in terms of age, gender, tumor size, tumor laterality, tumor location, histologic diagnosis, and institution (Supplemental table S1). \n",
    "\n",
    "Performance characteristics of the clinical variable logistic regression, models trained on T1C and T2WI images, and the final ensemble model in test set are summarized in Table 2. Performance characteristics in training and validation sets are summarized in Supplemental table S2. The clinical variable logistic regression achieved a test accuracy of {regression_accuracy}, F1 score of {regression_F1}, precision recall AUC of {regression_PR_AUC}, sensitivity of {regression_sensitivity}, and specificity of {regression_specificity}. \n",
    "\n",
    "The T1C trained model achieved a test accuracy of {t1c_accuracy}, F1 score of {t1c_f1}, precision recall AUC of {t1c_PR_AUC}, sensitivity of 0.92 (95% CI: 0.78-0.98), and specificity of 0.50 (95% CI: 0.24-0.76). The T2WI trained model achieved a test accuracy of 0.83 (95% CI: 0.70-0.91), F1 score of 0.90, precision recall AUC of 0.92, sensitivity of 0.97 (95% CI: 0.84-1.00), and specificity of 0.30 (95% CI: 0.10-0.61). The ensemble model achieved a test accuracy of 0.89 (95% CI: 0.77-0.96), F1 score of 0.94, precision recall AUC of 0.90, sensitivity of 1.0 (95% CI: 0.88-1.00), and specificity of 0.50 (95% CI: 0.24-0.76). The ensemble model achieved comparative performance on the second set of test set segmentations with an accuracy of 0.81 (95% CI: 0.67-0.90), F1 score of 0.89, precision recall AUC of 0.89, sensitivity of 0.97 (95% CI: 0.84-1.00), and specificity of 0.20 (95% CI: 0.05-0.52) and the third set of test set segmentation with an accuracy of 0.81 (95% CI: 0.67-0.90), F1 score of 0.89, precision recall AUC of 0.83, sensitivity of 1.0 (95% CI: 0.88-1.00), and specificity of 0.10 (95% CI: 0.00-0.42) (Supplemental table S3). The average cross validation test accuracy was 0.80 (95% CI: 0.66-0.89) with average F1 score of 0.88, precision recall AUC of 0.90, sensitivity of 0.96 (95% CI: 0.83-1.00), and specificity of 0.22 (95% CI: 0.06-0.54). Supplemental table S4 summarizes the cross-validation performance of the ensemble model.\n",
    "\n",
    "In comparison, expert 1 achieved a test accuracy of 0.85 (95% CI: 0.72-0.92), F1 score of 0.91, and sensitivity of 1.0 (95% CI: 0.88-1.00), and specificity of 0.30 (95% CI: 0.10-0.61); expert 2 had a test accuracy of 0.79 (95% CI: 0.65-0.88), F1 score of 0.88, and sensitivity of 0.95 (95% CI: 0.82-1.00), and specificity of 0.20 (95% CI: 0.05-0.52); expert 3 had a test accuracy of 0.74 (95% CI: 0.60-0.84), F1 score of 0.73, sensitivity of 0.81 (95% CI: 0.65-0.90), and specificity of 0.50 (95% CI: 0.24-0.76); expert 4 had a test accuracy of 0.85 (95% CI: 0.72-0.93), F1 score of 0.91, and sensitivity of 0.92 (95% CI: 0.78 -0.98), and specificity of 0.60 (95% CI: 0.31-0.83). Radiomics model achieved a test accuracy of 0.78 (95% CI: 0.64-0.88), F1 score of 0.84, sensitivity of 0.76 (95% CI: 0.60-0.87), and specificity of 0.80 (95% CI: 0.47-0.95) (Supplemental table S5).\n",
    "\n",
    "Compared to all experts averaged, the ensemble deep learning model had higher test accuracy (0.89 vs. 0.81, p=0.32), higher test sensitivity (1.00 vs. 0.92, p=0.07) and higher test specificity (0.50 vs. 0.40, p=0.53), although none of these statistics was significantly different. Compared to the radiomics model, the ensemble deep learning model had higher test accuracy (0.89 vs. 0.77, p=0.12), higher test sensitivity (1.00 vs. 0.76, p<0.01) and lower test specificity (0.50 vs. 0.80, p=0.03); difference in accuracy was not significant but differences in sensitivity and specificity between radiomics and ensemble deep learning models were significant. Compared to all experts averaged, the radiomics model had lower test accuracy (0.77 vs. 0.81, p=0.61), lower test sensitivity (0.76 vs. 0.92, p=0.0021) and higher test specificity (0.80 vs. 0.40, p<0.01); difference in accuracy was not significant, but differences in sensitivity and specificity between radiomics and averaged expert performance metrics were significant. Figure 2 shows the precision recall curves of all models overlaid with expert performance. \n",
    "\n",
    "Grad-Cam focus maps demonstrate that for the correctly classified images by our model, the algorithm was more likely to focus on the lesion or part of the lesion deemed important by the radiologists, while focus was more likely to be on the surrounding tissue for the incorrectly classified lesions (Figure 3). t-SNE representation of the final dense layer of ResNet demonstrates good separation of malignant and benign lesions by the model when compared to histopathological diagnosis (Figure 4).\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_multiple_precision_recall(train)#, experts=get_experts_for_names(expert_features, train_set.names))#, comparison_models=get_experts_for_names(comparison_model_features, train_set.names, experts=['radiomics'], transform=float))\n",
    "fig.savefig(\"figures/combined-train-precision-recall.svg\", bbox_inches = \"tight\")\n",
    "fig = plot_multiple_precision_recall(validation)#, experts=get_experts_for_names(expert_features, validation_set.names))#,  comparison_models=get_experts_for_names(comparison_model_features, validation_set.names, experts=['radiomics'], transform=float))\n",
    "fig.savefig(\"figures/combined-validation-precision-recall.svg\", bbox_inches = \"tight\")\n",
    "fig = plot_multiple_precision_recall(test, experts=get_experts_for_names(expert_features, test_set.names, experts=[\"expert1\", \"expert2\", \"expert3\", \"expert4\", 'radiomics1', 'radiomics2']))#, comparison_models=get_experts_for_names(expert_features, test_set.names, experts=['radiomics1', 'radiomics2'], transform=float))\n",
    "fig.savefig(\"figures/combined-test-precision-recall.svg\", bbox_inches = \"tight\")\n",
    "fig = plot_multiple_roc_curve(train) #, experts=get_experts_for_names(expert_features, train_set.names))#, comparison_models=get_experts_for_names(comparison_model_features, train_set.names, experts=['radiomics'], transform=float))\n",
    "fig.savefig(\"figures/combined-train-roc.svg\", bbox_inches = \"tight\")\n",
    "fig = plot_multiple_roc_curve(validation)#, experts=get_experts_for_names(expert_features, validation_set.names))#, comparison_models=get_experts_for_names(comparison_model_features, validation_set.names, experts=['radiomics'], transform=float))\n",
    "fig.savefig(\"figures/combined-validation-roc.svg\", bbox_inches = \"tight\")\n",
    "fig = plot_multiple_roc_curve(test, experts=get_experts_for_names(expert_features, test_set.names, experts=[\"expert1\", \"expert2\", \"expert3\", \"expert4\", 'radiomics1', 'radiomics2'])) #, comparison_models=get_experts_for_names(expert_features, test_set.names, experts=['radiomics1', 'radiomics2'], transform=float))\n",
    "fig.savefig(\"figures/combined-test-roc.svg\", bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_num_for_modality(dataset, experts=[], comparison_models=[]): \n",
    "    results = list()\n",
    "    if len(experts) > 0:\n",
    "        for i, expert in enumerate(experts): \n",
    "            labels = dataset[\"t1-labels\"]\n",
    "            predictions = expert\n",
    "            correct = sum([ labels[i] == p for i, p in enumerate(predictions) ])\n",
    "            total = len(labels)\n",
    "            results.append({ \n",
    "                \"correct\": correct, \n",
    "                \"total\": total, \n",
    "                \"incorrect\": total-correct, \n",
    "                \"modality\": \"Expert {}\".format(i),\n",
    "                \"acc\": correct/total,                \n",
    "                \"wald\": adjusted_wald(correct/total, total),\n",
    "                **calculate_confusion_matrix_stats_predictions(labels, predictions),\n",
    "            })\n",
    "    for modality in MODALITIES: \n",
    "        labels = dataset[\"{}-labels\".format(modality)]\n",
    "        probabilities = dataset[\"{}-probabilities\".format(modality)]\n",
    "        predictions = dataset[\"{}-predictions\".format(modality)]\n",
    "        total = len(labels)\n",
    "        correct = sum([ labels[i] == p for i, p in enumerate(predictions) ])\n",
    "        results.append({ \n",
    "            \"correct\": correct, \n",
    "            \"total\": total, \n",
    "            \"incorrect\": total-correct, \n",
    "            \"modality\": MODALITY_KEY[modality],\n",
    "            \"acc\": correct/total,\n",
    "            \"wald\": adjusted_wald(correct/total, total),\n",
    "            **calculate_confusion_matrix_stats_predictions(labels, predictions),            \n",
    "        })        \n",
    "    for probabilities in comparison_models: \n",
    "        modality = \"Radiomics\"\n",
    "        labels = dataset[\"t1-labels\"]\n",
    "        predictions = [p > 0.5 for p in probabilities]\n",
    "        predictions = dataset[\"{}-predictions\".format(modality)]\n",
    "        total = len(labels)\n",
    "        correct = sum([ labels[i] == p for i, p in enumerate(predictions) ])\n",
    "        results.append({ \n",
    "            \"correct\": correct, \n",
    "            \"total\": total, \n",
    "            \"incorrect\": total-correct, \n",
    "            \"modality\": modality,\n",
    "            \"acc\": correct/total,\n",
    "            \"wald\": adjusted_wald(correct/total, total),\n",
    "            **calculate_confusion_matrix_stats_predictions(labels, predictions),            \n",
    "        })                \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = correct_num_for_modality(test)#, experts=get_experts_for_names(expert_features, test_set.names, experts=[\"expert1\", \"expert2\", \"expert3\", \"expert4\"]))#, comparison_models=get_experts_for_names(comparison_model_features, test_set.names, experts=['radiomics'], transform=float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_scores = list()\n",
    "for e in experts: \n",
    "    current = list()\n",
    "    for x in experts: \n",
    "        current.append(cohen_kappa_score(e, x))\n",
    "    kappa_scores.append(current)\n",
    "kappa_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.heatmap(kappa_scores, annot=True, square=True, fmt=\".2f\", xticklabels=[\"1\", \"2\", \"3\", \"4\"], yticklabels=[\"1\", \"2\", \"3\", \"4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_kappa = list()\n",
    "for x, y in combinations(experts, 2): \n",
    "    unique_kappa.append(cohen_kappa_score(x, y))\n",
    "np.average(unique_kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fleiss_kappa(ratings, n):\n",
    "    '''\n",
    "    Computes the Fleiss' kappa measure for assessing the reliability of \n",
    "    agreement between a fixed number n of raters when assigning categorical\n",
    "    ratings to a number of items.\n",
    "    \n",
    "    Args:\n",
    "        ratings: a list of (item, category)-ratings\n",
    "        n: number of raters\n",
    "        k: number of categories\n",
    "    Returns:\n",
    "        the Fleiss' kappa score\n",
    "    \n",
    "    See also:\n",
    "        http://en.wikipedia.org/wiki/Fleiss'_kappa\n",
    "    '''\n",
    "    items = set()\n",
    "    categories = set()\n",
    "    n_ij = {}\n",
    "    \n",
    "    for i, c in ratings:\n",
    "        items.add(i)\n",
    "        categories.add(c)\n",
    "        n_ij[(i,c)] = n_ij.get((i,c), 0) + 1\n",
    "    \n",
    "    N = len(items)\n",
    "    \n",
    "    p_j = dict(((c, sum(n_ij.get((i, c), 0) for i in items) / (1.0 * n * N)) for c in categories))\n",
    "    P_i = dict(((i, (sum(n_ij.get((i, c), 0) ** 2 for c in categories) - n) / (n * (n - 1.0))) for i in items))\n",
    "\n",
    "    P_bar = sum(P_i.values()) / (1.0 * N)\n",
    "    P_e_bar = sum(value ** 2 for value in p_j.values())\n",
    "    \n",
    "    kappa = (P_bar - P_e_bar) / (1 - P_e_bar)\n",
    "    \n",
    "    return kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fleiss_kappa_array = list()\n",
    "for e in list(zip(*experts)): \n",
    "    for i in e: \n",
    "        fleiss_kappa_array.append((1, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fleiss_kappa(fleiss_kappa_array, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = [(1, 'yes')] * 10 + [(1, 'no')] * 0  + \\\n",
    "[(2, 'yes')] * 8  + [(2, 'no')] * 2  + \\\n",
    "[(3, 'yes')] * 9  + [(3, 'no')] * 1  + \\\n",
    "[(4, 'yes')] * 0  + [(4, 'no')] * 10 + \\\n",
    "[(5, 'yes')] * 7  + [(5, 'no')] * 3\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(experts).T\n",
    "fleiss_kappa(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = results.pop()\n",
    "for r in results: \n",
    "    print(r[\"modality\"], fisher_exact([[ensemble[\"correct\"], r[\"correct\"]], [ensemble[\"incorrect\"], r[\"incorrect\"]]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
